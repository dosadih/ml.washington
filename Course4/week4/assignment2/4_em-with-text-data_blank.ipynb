{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a diagonal covariance Gaussian mixture model to text data\n",
    "\n",
    "In a previous assignment, we explored k-means clustering for a high-dimensional Wikipedia dataset. We can also model this data with a mixture of Gaussians, though with increasing dimension we run into two important issues associated with using a full covariance matrix for each component.\n",
    " * Computational cost becomes prohibitive in high dimensions: score calculations have complexity cubic in the number of dimensions M if the Gaussian has a full covariance matrix.\n",
    " * A model with many parameters require more data: observe that a full covariance matrix for an M-dimensional Gaussian will have M(M+1)/2 parameters to fit. With the number of parameters growing roughly as the square of the dimension, it may quickly become impossible to find a sufficient amount of data to make good inferences.\n",
    "\n",
    "Both of these issues are avoided if we require the covariance matrix of each component to be diagonal, as then it has only M parameters to fit and the score computation decomposes into M univariate score calculations. Recall from the lecture that the M-step for the full covariance is:\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\Sigma}_k &= \\frac{1}{N_k^{soft}} \\sum_{i=1}^N r_{ik} (x_i-\\hat{\\mu}_k)(x_i - \\hat{\\mu}_k)^T\n",
    "\\end{align*}\n",
    "\n",
    "Note that this is a square matrix with M rows and M columns, and the above equation implies that the (v, w) element is computed by\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\Sigma}_{k, v, w} &= \\frac{1}{N_k^{soft}} \\sum_{i=1}^N r_{ik} (x_{iv}-\\hat{\\mu}_{kv})(x_{iw} - \\hat{\\mu}_{kw})\n",
    "\\end{align*}\n",
    "\n",
    "When we assume that this is a diagonal matrix, then non-diagonal elements are assumed to be zero and we only need to compute each of the M elements along the diagonal independently using the following equation. \n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\sigma}^2_{k, v} &= \\hat{\\Sigma}_{k, v, v}  \\\\\n",
    "&= \\frac{1}{N_k^{soft}} \\sum_{i=1}^N r_{ik} (x_{iv}-\\hat{\\mu}_{kv})^2\n",
    "\\end{align*}\n",
    "\n",
    "In this section, we will use an EM implementation to fit a Gaussian mixture model with **diagonal** covariances to a subset of the Wikipedia dataset. The implementation uses the above equation to compute each variance term. \n",
    "\n",
    "We'll begin by importing the dataset and coming up with a useful representation for each article. After running our algorithm on the data, we will explore the output to see whether we can give a meaningful interpretation to the fitted parameters in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note to Amazon EC2 users**: To conserve memory, make sure to stop all the other notebooks before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block will check if you have the correct version of GraphLab Create. Any version later than 1.8.5 will do. To upgrade, read [this page](https://turi.com/download/upgrade-graphlab-create.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab\n",
    "\n",
    "'''Check GraphLab Create version'''\n",
    "from distutils.version import StrictVersion\n",
    "assert (StrictVersion(graphlab.version) >= StrictVersion('1.8.5')), 'GraphLab Create must be version 1.8.5 or later.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have a Python file containing implementations for several functions that will be used during the course of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from em_utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Wikipedia data and extract TF-IDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Wikipedia data and transform each of the first 5000 document into a TF-IDF representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to damien.pretet@me.com and will expire on September 17, 2017.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: /tmp/graphlab_server_1483177012.log\n"
     ]
    }
   ],
   "source": [
    "wiki = graphlab.SFrame('people_wiki.gl/').head(5000)\n",
    "wiki['tf_idf'] = graphlab.text_analytics.tf_idf(wiki['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a utility we provide, we will create a sparse matrix representation of the documents. This is the same utility function you used during the previous assignment on k-means with text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf, map_index_to_word = sframe_to_scipy(wiki, 'tf_idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous assignment, we will normalize each document's TF-IDF vector to be a unit vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf_idf = normalize(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the length (Euclidean norm) of each row is now 1.0, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    doc = tf_idf[i]\n",
    "    print(np.linalg.norm(doc.todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM in high dimensions\n",
    "\n",
    "EM for high-dimensional data requires some special treatment:\n",
    " * E step and M step must be vectorized as much as possible, as explicit loops are dreadfully slow in Python.\n",
    " * All operations must be cast in terms of sparse matrix operations, to take advantage of computational savings enabled by sparsity of data.\n",
    " * Initially, some words may be entirely absent from a cluster, causing the M step to produce zero mean and variance for those words.  This means any data point with one of those words will have 0 probability of being assigned to that cluster since the cluster allows for no variability (0 variance) around that count being 0 (0 mean). Since there is a small chance for those words to later appear in the cluster, we instead assign a small positive variance (~1e-10). Doing so also prevents numerical overflow.\n",
    " \n",
    "We provide the complete implementation for you in the file `em_utilities.py`. For those who are interested, you can read through the code to see how the sparse matrix implementation differs from the previous assignment. \n",
    "\n",
    "You are expected to answer some quiz questions using the results of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing mean parameters using k-means**\n",
    "\n",
    "Recall from the lectures that EM for Gaussian mixtures is very sensitive to the choice of initial means. With a bad initial set of means, EM may produce clusters that span a large area and are mostly overlapping. To eliminate such bad outcomes, we first produce a suitable set of initial means by using the cluster centers from running k-means.  That is, we first run k-means and then take the final set of means from the converged solution as the initial means in our EM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.random.seed(5)\n",
    "num_clusters = 25\n",
    "\n",
    "# Use scikit-learn's k-means to simplify workflow\n",
    "#kmeans_model = KMeans(n_clusters=num_clusters, n_init=5, max_iter=400, random_state=1, n_jobs=-1) # uncomment to use parallelism -- may break on your installation\n",
    "kmeans_model = KMeans(n_clusters=num_clusters, n_init=5, max_iter=400, random_state=1, n_jobs=1)\n",
    "kmeans_model.fit(tf_idf)\n",
    "centroids, cluster_assignment = kmeans_model.cluster_centers_, kmeans_model.labels_\n",
    "\n",
    "means = [centroid for centroid in centroids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing cluster weights**\n",
    "\n",
    "We will initialize each cluster weight to be the proportion of documents assigned to that cluster by k-means above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_docs = tf_idf.shape[0]\n",
    "weights = []\n",
    "for i in xrange(num_clusters):\n",
    "    # Compute the number of data points assigned to cluster i:\n",
    "    num_assigned = len(centroids[i]) # YOUR CODE HERE\n",
    "    w = float(num_assigned) / num_docs\n",
    "    weights.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initializing covariances**\n",
    "\n",
    "To initialize our covariance parameters, we compute $\\hat{\\sigma}_{k, j}^2 = \\sum_{i=1}^{N}(x_{i,j} - \\hat{\\mu}_{k, j})^2$ for each feature $j$.  For features with really tiny variances, we assign 1e-8 instead to prevent numerical instability. We do this computation in a vectorized fashion in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "covs = []\n",
    "for i in xrange(num_clusters):\n",
    "    member_rows = tf_idf[cluster_assignment==i]\n",
    "    cov = (member_rows.multiply(member_rows) - 2*member_rows.dot(diag(means[i]))).sum(axis=0).A1 / member_rows.shape[0] \\\n",
    "          + means[i]**2\n",
    "    cov[cov < 1e-8] = 1e-8\n",
    "    covs.append(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running EM**\n",
    "\n",
    "Now that we have initialized all of our parameters, run EM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out = EM_for_high_dimension(tf_idf, means, covs, weights, cov_smoothing=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3879327582.0772448, 4883345753.5331306, 4883345753.5331306]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['loglik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loglik': [3879327582.0772448, 4883345753.5331306, 4883345753.5331306], 'resp': array([[ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]]), 'covs': [array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         6.51877866e-09,   3.35273163e-09,   1.39388715e-09]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         9.03175914e-09,   2.98528202e-09,   1.02418143e-09]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         8.11103040e-09,   1.40877532e-09,   9.29567270e-10]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         6.40751196e-09,   2.59161887e-09,   8.83380254e-10]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         8.04873735e-09,   2.39484591e-09,   1.19487932e-09]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         7.78973227e-09,   1.78311757e-09,   1.18406753e-09]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         7.76283763e-09,   1.44231712e-09,   1.04035900e-09]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         9.33954538e-09,   2.59919024e-09,   1.13289555e-09]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         9.85042809e-09,   3.39051765e-09,   6.78321255e-10]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         6.24485996e-09,   2.87929958e-09,   1.08120493e-09]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         8.94942536e-09,   2.86328189e-09,   9.24960665e-10]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         8.17405306e-09,   3.56157974e-09,   1.16412334e-09]), array([  1.00000000e-10,   1.00000000e-10,   7.76577387e-05, ...,\n",
      "         6.88620064e-09,   3.43609243e-09,   1.45977425e-09]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         7.77031268e-09,   1.31533903e-09,   1.08287930e-09]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         7.20227237e-09,   1.70141415e-09,   9.10968065e-10]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         5.88306287e-09,   1.55549588e-09,   7.38351767e-10]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         8.40515384e-09,   2.54659933e-09,   1.14038937e-09]), array([  3.48996693e-05,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         6.10124132e-09,   1.90977348e-09,   8.58486201e-10]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         8.31941434e-09,   3.40905999e-09,   1.10737253e-09]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         7.98132650e-09,   1.48648704e-09,   8.27827454e-10]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         6.32281457e-09,   1.96004129e-09,   8.91974444e-10]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         6.77003723e-09,   3.08522435e-09,   1.11139793e-09]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         1.06200039e-08,   2.48762127e-09,   1.05453936e-09]), array([  1.00000000e-10,   1.00000000e-10,   1.00000000e-10, ...,\n",
      "         6.98751004e-09,   2.21618136e-09,   1.03778108e-09]), array([  1.00000000e-10,   2.59918986e-05,   1.00000000e-10, ...,\n",
      "         5.55163776e-09,   2.47025469e-09,   9.61968829e-10])], 'weights': array([ 0.009 ,  0.081 ,  0.0536,  0.0336,  0.036 ,  0.014 ,  0.0412,\n",
      "        0.0256,  0.0104,  0.066 ,  0.0282,  0.0912,  0.0264,  0.0256,\n",
      "        0.035 ,  0.0214,  0.0142,  0.1218,  0.0308,  0.0178,  0.0272,\n",
      "        0.038 ,  0.0412,  0.0392,  0.0716]), 'means': [array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         1.79942936e-04,   1.38140667e-04,   7.52519518e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.23579792e-04,   1.31387094e-04,   8.53687044e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.57794488e-04,   9.10226593e-05,   8.63707823e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.15889678e-04,   1.00662936e-04,   9.09111720e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         1.90336708e-04,   1.39094073e-04,   8.35359900e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.34408669e-04,   8.79722226e-05,   9.68849648e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.43089838e-04,   8.02849784e-05,   8.49266981e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.07034151e-04,   1.40097040e-04,   9.10716403e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.05112230e-04,   1.39280540e-04,   7.45475368e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         1.76996360e-04,   1.30430940e-04,   8.16364264e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.21846594e-04,   1.47403392e-04,   8.45760373e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.06149890e-04,   1.55238693e-04,   9.35933016e-05]), array([  0.00000000e+00,   0.00000000e+00,   7.69939220e-04, ...,\n",
      "         2.02029547e-04,   1.25717072e-04,   9.33672496e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.07547516e-04,   8.31657974e-05,   1.07994050e-04]), array([ 0.        ,  0.        ,  0.        , ...,  0.00025411,\n",
      "        0.00010377,  0.00010457]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.07576195e-04,   1.03214956e-04,   9.27617511e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.22518434e-04,   1.11184683e-04,   9.47769141e-05]), array([  2.39584239e-04,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         1.66383200e-04,   1.02229585e-04,   7.45341124e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         1.91815412e-04,   1.56645470e-04,   9.46711655e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.68211934e-04,   8.74442072e-05,   9.76059782e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.03583915e-04,   1.13154052e-04,   9.46074606e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         1.81351373e-04,   1.37846017e-04,   8.12510991e-05]), array([  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
      "         2.27670892e-04,   1.43313323e-04,   9.40545820e-05]), array([ 0.        ,  0.        ,  0.        , ...,  0.00020392,\n",
      "        0.00010381,  0.00010233]), array([  0.00000000e+00,   2.69826164e-04,   0.00000000e+00, ...,\n",
      "         1.63292080e-04,   1.35964683e-04,   7.72546921e-05])]}\n"
     ]
    }
   ],
   "source": [
    "print out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret clustering results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to k-means, EM is able to explicitly model clusters of varying sizes and proportions. The relative magnitude of variances in the word dimensions tell us much about the nature of the clusters.\n",
    "\n",
    "Write yourself a cluster visualizer as follows.  Examining each cluster's mean vector, list the 5 words with the largest mean values (5 most common words in the cluster). For each word, also include the associated variance parameter (diagonal element of the covariance matrix). \n",
    "\n",
    "A sample output may be:\n",
    "```\n",
    "==========================================================\n",
    "Cluster 0: Largest mean parameters in cluster \n",
    "\n",
    "Word        Mean        Variance    \n",
    "football    1.08e-01    8.64e-03\n",
    "season      5.80e-02    2.93e-03\n",
    "club        4.48e-02    1.99e-03\n",
    "league      3.94e-02    1.08e-03\n",
    "played      3.83e-02    8.45e-04\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------------+-------+\n",
      "| feature |          category          | index |\n",
      "+---------+----------------------------+-------+\n",
      "| feature |         conflating         |   0   |\n",
      "| feature |          diamono           |   1   |\n",
      "| feature |          bailouts          |   2   |\n",
      "| feature |        electionruss        |   3   |\n",
      "| feature |          maywoods          |   4   |\n",
      "| feature |          feduring          |   5   |\n",
      "| feature |         spiderbait         |   6   |\n",
      "| feature |            mcin            |   7   |\n",
      "| feature |         sumiswald          |   8   |\n",
      "| feature |           quinta           |   9   |\n",
      "| feature |            749             |   10  |\n",
      "| feature |        firedheading        |   11  |\n",
      "| feature |          hatchett          |   12  |\n",
      "| feature |          drumsshe          |   13  |\n",
      "| feature |         2001prior          |   14  |\n",
      "| feature |        selingerborn        |   15  |\n",
      "| feature |            lavo            |   16  |\n",
      "| feature |            oiwa            |   17  |\n",
      "| feature |          ranxerox          |   18  |\n",
      "| feature |          burenitu          |   19  |\n",
      "| feature |           lezhan           |   20  |\n",
      "| feature |        regulationhe        |   21  |\n",
      "| feature |          outgroup          |   22  |\n",
      "| feature |            dani            |   23  |\n",
      "| feature |        melbournethe        |   24  |\n",
      "| feature |        mccarthyhis         |   25  |\n",
      "| feature |       folkebevegelse       |   26  |\n",
      "| feature |           quads            |   27  |\n",
      "| feature |          joschka           |   28  |\n",
      "| feature |        anticorrida         |   29  |\n",
      "| feature |        unstretched         |   30  |\n",
      "| feature |           amlia            |   31  |\n",
      "| feature |        rheinfelden         |   32  |\n",
      "| feature |            loch            |   33  |\n",
      "| feature |      minutescompeting      |   34  |\n",
      "| feature |         willpower          |   35  |\n",
      "| feature |        directorshis        |   36  |\n",
      "| feature |       eventtucholsky       |   37  |\n",
      "| feature |       mayorincumbent       |   38  |\n",
      "| feature |            sane            |   39  |\n",
      "| feature |          topshops          |   40  |\n",
      "| feature |           civico           |   41  |\n",
      "| feature |            poc             |   42  |\n",
      "| feature |          tucuman           |   43  |\n",
      "| feature |        comendadora         |   44  |\n",
      "| feature |         vaultlike          |   45  |\n",
      "| feature |            atay            |   46  |\n",
      "| feature |         ennobling          |   47  |\n",
      "| feature |    turangalilasymphony     |   48  |\n",
      "| feature |           roxas            |   49  |\n",
      "| feature |           slagle           |   50  |\n",
      "| feature |         volkovain          |   51  |\n",
      "| feature |        pellatfinet         |   52  |\n",
      "| feature |           goths            |   53  |\n",
      "| feature |       albumscorreia        |   54  |\n",
      "| feature |          visqueen          |   55  |\n",
      "| feature |          airfoil           |   56  |\n",
      "| feature |         piemontese         |   57  |\n",
      "| feature |          tourten           |   58  |\n",
      "| feature |    actorwriterdirector     |   59  |\n",
      "| feature |         critiqueof         |   60  |\n",
      "| feature |           acegas           |   61  |\n",
      "| feature |     editorinchiefsoon      |   62  |\n",
      "| feature |           wilby            |   63  |\n",
      "| feature |          chereko           |   64  |\n",
      "| feature |          loggins           |   65  |\n",
      "| feature |           skyrme           |   66  |\n",
      "| feature |          area1the          |   67  |\n",
      "| feature |          datacomm          |   68  |\n",
      "| feature |        presenteron         |   69  |\n",
      "| feature |        hostpathogen        |   70  |\n",
      "| feature |         thirdwave          |   71  |\n",
      "| feature |           spens            |   72  |\n",
      "| feature |      trandafirivocal       |   73  |\n",
      "| feature |         edmundshis         |   74  |\n",
      "| feature |        ulaanbaatar         |   75  |\n",
      "| feature |         teampeter          |   76  |\n",
      "| feature |          lordshe           |   77  |\n",
      "| feature |            svit            |   78  |\n",
      "| feature |          modugno           |   79  |\n",
      "| feature |            juts            |   80  |\n",
      "| feature |         noncitizen         |   81  |\n",
      "| feature |           mtiers           |   82  |\n",
      "| feature |            koll            |   83  |\n",
      "| feature |       designfashion        |   84  |\n",
      "| feature |         stanfordin         |   85  |\n",
      "| feature |          baffles           |   86  |\n",
      "| feature | computeraidedtranscription |   87  |\n",
      "| feature |         haderslev          |   88  |\n",
      "| feature |           sellar           |   89  |\n",
      "| feature |          hailpin           |   90  |\n",
      "| feature |         unicycles          |   91  |\n",
      "| feature |         kortekaas          |   92  |\n",
      "| feature |         scarbrough         |   93  |\n",
      "| feature |          xiaodong          |   94  |\n",
      "| feature |           cumnor           |   95  |\n",
      "| feature |        villafranco         |   96  |\n",
      "| feature |            welu            |   97  |\n",
      "| feature |            nabo            |   98  |\n",
      "| feature |        haralabidou         |   99  |\n",
      "| feature |         xvfelsted          |  100  |\n",
      "| feature |           binge            |  101  |\n",
      "| feature |           pivnik           |  102  |\n",
      "| feature |            epah            |  103  |\n",
      "| feature |            ngg             |  104  |\n",
      "| feature |      choreographymann      |  105  |\n",
      "| feature |         caltechhe          |  106  |\n",
      "| feature |          teruyuki          |  107  |\n",
      "| feature |           jitan            |  108  |\n",
      "| feature |           djabi            |  109  |\n",
      "| feature |          ppeared           |  110  |\n",
      "| feature |       futurecollins        |  111  |\n",
      "| feature |          thereshe          |  112  |\n",
      "| feature |           chole            |  113  |\n",
      "| feature |      settlementafter       |  114  |\n",
      "| feature |           najars           |  115  |\n",
      "| feature |       selfappointed        |  116  |\n",
      "| feature |           krohn            |  117  |\n",
      "| feature |         longstreet         |  118  |\n",
      "| feature |      seinesaintdenis       |  119  |\n",
      "| feature |         wulfdieter         |  120  |\n",
      "| feature |          qinghai           |  121  |\n",
      "| feature |          hayeshe           |  122  |\n",
      "| feature |           mozos            |  123  |\n",
      "| feature |      decommissioning       |  124  |\n",
      "| feature |         diabolical         |  125  |\n",
      "| feature |            zdna            |  126  |\n",
      "| feature |           tansky           |  127  |\n",
      "| feature |          koreaon           |  128  |\n",
      "| feature |           karts            |  129  |\n",
      "| feature |          ecuadors          |  130  |\n",
      "| feature |        fourthdegree        |  131  |\n",
      "| feature |        recalibrate         |  132  |\n",
      "| feature |         usjapanese         |  133  |\n",
      "| feature |        rzewskisince        |  134  |\n",
      "| feature |          rasheeds          |  135  |\n",
      "| feature |        ordentlicher        |  136  |\n",
      "| feature |         coproposed         |  137  |\n",
      "| feature |        playerburns         |  138  |\n",
      "| feature |       1950gruenberg        |  139  |\n",
      "| feature |          quannum           |  140  |\n",
      "| feature |           alloys           |  141  |\n",
      "| feature |          osmania           |  142  |\n",
      "| feature |           13808            |  143  |\n",
      "| feature |           nisshi           |  144  |\n",
      "| feature |       quartermaster        |  145  |\n",
      "| feature |         hanseatic          |  146  |\n",
      "| feature |     footballeredwards      |  147  |\n",
      "| feature |       calvisanomark        |  148  |\n",
      "| feature |         suissebrm          |  149  |\n",
      "| feature |         recoveryin         |  150  |\n",
      "| feature |       reinforcements       |  151  |\n",
      "| feature |       housingrelated       |  152  |\n",
      "| feature |         abrahamic          |  153  |\n",
      "| feature |            eyal            |  154  |\n",
      "| feature |        southerners         |  155  |\n",
      "| feature |        dobbersafter        |  156  |\n",
      "| feature |         lindfjeld          |  157  |\n",
      "| feature |          coachan           |  158  |\n",
      "| feature |         advocacies         |  159  |\n",
      "| feature |         novecento          |  160  |\n",
      "| feature |          asherton          |  161  |\n",
      "| feature |           eular            |  162  |\n",
      "| feature |        cbssportscom        |  163  |\n",
      "| feature |        consequenses        |  164  |\n",
      "| feature |      theatreworksusas      |  165  |\n",
      "| feature |           rorty            |  166  |\n",
      "| feature |         unionnaxos         |  167  |\n",
      "| feature |          karpovof          |  168  |\n",
      "| feature |      connectiondaisy       |  169  |\n",
      "| feature |            uipm            |  170  |\n",
      "| feature |           germs            |  171  |\n",
      "| feature |        forestethics        |  172  |\n",
      "| feature |          wallaby           |  173  |\n",
      "| feature |          heretics          |  174  |\n",
      "| feature |         forsunnet          |  175  |\n",
      "| feature |         neurologic         |  176  |\n",
      "| feature |           2015as           |  177  |\n",
      "| feature |          tehatre           |  178  |\n",
      "| feature |          polamar           |  179  |\n",
      "| feature |           protti           |  180  |\n",
      "| feature |          hacarmel          |  181  |\n",
      "| feature |       tvfilmohmphrey       |  182  |\n",
      "| feature |       nicknamestage        |  183  |\n",
      "| feature |          awarddr           |  184  |\n",
      "| feature |        improvsketch        |  185  |\n",
      "| feature |            iie             |  186  |\n",
      "| feature |         chwerwder          |  187  |\n",
      "| feature |          moselle           |  188  |\n",
      "| feature |           sijie            |  189  |\n",
      "| feature |      jagannathkrishna      |  190  |\n",
      "| feature |       vauclusedoyle        |  191  |\n",
      "| feature |        vflbrittain         |  192  |\n",
      "| feature |           miesha           |  193  |\n",
      "| feature |           kirotv           |  194  |\n",
      "| feature |           linsly           |  195  |\n",
      "| feature |          alwaleed          |  196  |\n",
      "| feature |         monstrous          |  197  |\n",
      "| feature |           hoaxin           |  198  |\n",
      "| feature |        shamankings         |  199  |\n",
      "+---------+----------------------------+-------+\n",
      "[100282 rows x 3 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "map_index_to_word.print_rows(num_rows=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50140 66602 66601 ...,  1464  2327  1237]\n",
      "[50140 65444 65443 ...,   108    83    90]\n",
      "[50140 66274 66273 ...,   150   191   300]\n",
      "[50140 66398 66397 ...,   832  1103   425]\n",
      "[50140 66196 66195 ...,   145   153   796]\n",
      "[50140 66674 66673 ...,  1050   134  1586]\n",
      "[50140 66254 66253 ...,   371  3850   483]\n",
      "[50140 66357 66356 ...,   351   409   162]\n",
      "[100281  63810  63809 ...,   7075   4556    892]\n",
      "[50140 65704 65703 ...,   519   254   775]\n",
      "[50140 66322 66321 ...,   289   789   291]\n",
      "[50140 65236 65234 ...,   158    24   197]\n",
      "[50140 66455 66454 ...,   831   430   250]\n",
      "[50140 66595 66594 ...,  1445  1720   379]\n",
      "[50140 66463 66462 ...,   161   150   673]\n",
      "[50140 66632 66631 ...,  3490   300  1146]\n",
      "[50140 66655 66654 ...,  4010  2340   815]\n",
      "[50140 49773 49772 ...,    69   245    21]\n",
      "[50140 66069 66068 ...,  1380   837   108]\n",
      "[50140 66620 66619 ...,  1739   598    90]\n",
      "[50140 66400 66399 ...,   231   938   451]\n",
      "[50140 66263 66262 ...,   199   248   433]\n",
      "[50140 66180 66179 ...,   559   162   538]\n",
      "[50140 66351 66350 ...,   225   330   242]\n",
      "[50140 65372 65371 ...,   108   335   303]\n"
     ]
    }
   ],
   "source": [
    "_means = out[\"means\"]\n",
    "num_clusters = len(_means)\n",
    "for c in xrange(num_clusters):\n",
    "    print np.argsort(_means[c][::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill in the blanks\n",
    "def visualize_EM_clusters(tf_idf, means, covs, map_index_to_word):\n",
    "    print('')\n",
    "    print('==========================================================')\n",
    "\n",
    "    num_clusters = len(means)\n",
    "    for c in xrange(num_clusters):\n",
    "        print('Cluster {0:d}: Largest mean parameters in cluster '.format(c))\n",
    "        print('\\n{0: <12}{1: <12}{2: <12}'.format('Word', 'Mean', 'Variance'))\n",
    "        \n",
    "        # The k'th element of sorted_word_ids should be the index of the word \n",
    "        # that has the k'th-largest value in the cluster mean. Hint: Use np.argsort().\n",
    "        sorted_word_ids = np.argsort(means[c][::])\n",
    "\n",
    "        for i in sorted_word_ids[-5:]:\n",
    "            print '{0: <12}{1:<10.2e}{2:10.2e}'.format(map_index_to_word['category'][i], \n",
    "                                                       means[c][i],\n",
    "                                                       covs[c][i])\n",
    "        print '\\n=========================================================='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================\n",
      "Cluster 0: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "literary    4.68e-02    3.29e-03\n",
      "de          4.77e-02    8.72e-03\n",
      "poet        5.91e-02    6.36e-03\n",
      "poems       6.33e-02    6.45e-03\n",
      "poetry      1.51e-01    1.90e-02\n",
      "\n",
      "==========================================================\n",
      "Cluster 1: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "show        1.27e-02    7.33e-04\n",
      "actress     1.52e-02    1.14e-03\n",
      "music       1.53e-02    1.04e-03\n",
      "her         1.04e-01    3.20e-03\n",
      "she         1.60e-01    4.59e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 2: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "played      3.79e-02    9.46e-04\n",
      "season      5.06e-02    2.35e-03\n",
      "league      5.72e-02    2.83e-03\n",
      "club        5.84e-02    2.55e-03\n",
      "football    7.45e-02    4.57e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 3: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "house       3.65e-02    2.07e-03\n",
      "democratic  3.72e-02    2.46e-03\n",
      "senate      5.04e-02    5.21e-03\n",
      "republican  5.47e-02    4.55e-03\n",
      "district    5.56e-02    4.00e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 4: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "fiction     3.06e-02    3.10e-03\n",
      "books       3.19e-02    1.57e-03\n",
      "published   3.87e-02    1.51e-03\n",
      "book        4.30e-02    2.38e-03\n",
      "novel       5.26e-02    5.50e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 5: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "cup         4.81e-02    2.30e-03\n",
      "played      4.90e-02    1.42e-03\n",
      "wales       4.96e-02    6.96e-03\n",
      "against     5.11e-02    1.99e-03\n",
      "rugby       1.59e-01    1.14e-02\n",
      "\n",
      "==========================================================\n",
      "Cluster 6: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "won         3.11e-02    6.93e-04\n",
      "racing      3.53e-02    4.77e-03\n",
      "championship4.31e-02    3.78e-03\n",
      "pga         4.42e-02    1.36e-02\n",
      "tour        5.18e-02    1.08e-02\n",
      "\n",
      "==========================================================\n",
      "Cluster 7: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "directed    2.83e-02    1.95e-03\n",
      "feature     3.41e-02    1.61e-03\n",
      "festival    4.46e-02    3.60e-03\n",
      "films       6.60e-02    3.33e-03\n",
      "film        1.78e-01    6.05e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 8: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "pianist     2.69e-02    2.30e-03\n",
      "music       3.39e-02    1.15e-03\n",
      "grandmaster 3.78e-02    8.97e-03\n",
      "chess       1.09e-01    3.10e-02\n",
      "jazz        1.62e-01    1.65e-02\n",
      "\n",
      "==========================================================\n",
      "Cluster 9: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "chairman    1.69e-02    1.51e-03\n",
      "company     1.80e-02    9.69e-04\n",
      "technology  1.90e-02    1.46e-03\n",
      "business    2.08e-02    1.55e-03\n",
      "engineering 2.15e-02    2.76e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 10: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "work        3.23e-02    7.92e-04\n",
      "gallery     3.91e-02    3.67e-03\n",
      "artist      4.17e-02    1.74e-03\n",
      "museum      6.21e-02    7.84e-03\n",
      "art         1.37e-01    6.78e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 11: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "studies     2.11e-02    1.76e-03\n",
      "science     2.31e-02    2.08e-03\n",
      "professor   3.30e-02    1.27e-03\n",
      "university  3.85e-02    7.99e-04\n",
      "research    4.17e-02    2.15e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 12: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "rights      3.47e-02    5.19e-03\n",
      "justice     3.63e-02    3.37e-03\n",
      "judge       4.00e-02    4.70e-03\n",
      "court       6.40e-02    5.53e-03\n",
      "law         1.13e-01    8.91e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 13: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "season      4.47e-02    1.93e-03\n",
      "nhl         6.11e-02    1.22e-02\n",
      "soccer      8.30e-02    2.46e-02\n",
      "hockey      9.28e-02    2.04e-02\n",
      "coach       9.60e-02    9.97e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 14: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "sox         4.57e-02    6.25e-03\n",
      "games       4.71e-02    1.94e-03\n",
      "major       5.10e-02    1.16e-03\n",
      "league      1.03e-01    3.64e-03\n",
      "baseball    1.15e-01    5.53e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 15: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "nfl         4.69e-02    7.80e-03\n",
      "nba         5.21e-02    8.68e-03\n",
      "yards       5.32e-02    1.39e-02\n",
      "football    5.98e-02    5.74e-03\n",
      "basketball  9.90e-02    1.32e-02\n",
      "\n",
      "==========================================================\n",
      "Cluster 16: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "lds         4.13e-02    9.88e-03\n",
      "diocese     4.70e-02    5.52e-03\n",
      "archbishop  4.87e-02    7.44e-03\n",
      "bishop      8.51e-02    1.06e-02\n",
      "church      1.20e-01    9.69e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 17: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "had         1.01e-02    2.24e-04\n",
      "he          1.03e-02    5.77e-05\n",
      "were        1.07e-02    2.83e-04\n",
      "i           1.19e-02    1.58e-03\n",
      "that        1.30e-02    1.97e-04\n",
      "\n",
      "==========================================================\n",
      "Cluster 18: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "opera       4.84e-02    1.30e-02\n",
      "conductor   4.97e-02    7.52e-03\n",
      "symphony    5.39e-02    8.05e-03\n",
      "orchestra   8.40e-02    9.74e-03\n",
      "music       1.12e-01    5.23e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 19: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "metres      5.87e-02    1.08e-02\n",
      "marathon    6.87e-02    2.49e-02\n",
      "miss        7.95e-02    2.83e-02\n",
      "championships8.06e-02    6.81e-03\n",
      "she         9.29e-02    8.21e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 20: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "cabinet     3.25e-02    3.34e-03\n",
      "party       3.32e-02    1.33e-03\n",
      "government  3.65e-02    2.00e-03\n",
      "prime       4.89e-02    4.79e-03\n",
      "minister    1.28e-01    7.34e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 21: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "reporter    2.38e-02    1.88e-03\n",
      "television  2.40e-02    1.07e-03\n",
      "show        3.08e-02    2.28e-03\n",
      "radio       4.04e-02    3.61e-03\n",
      "news        5.64e-02    6.42e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 22: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "comedy      3.52e-02    4.57e-03\n",
      "television  3.67e-02    1.74e-03\n",
      "actor       3.92e-02    3.01e-03\n",
      "film        4.28e-02    1.66e-03\n",
      "theatre     5.66e-02    7.14e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 23: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "council     3.18e-02    2.33e-03\n",
      "liberal     3.56e-02    5.48e-03\n",
      "elected     3.82e-02    1.24e-03\n",
      "election    6.29e-02    3.42e-03\n",
      "party       6.80e-02    3.06e-03\n",
      "\n",
      "==========================================================\n",
      "Cluster 24: Largest mean parameters in cluster \n",
      "\n",
      "Word        Mean        Variance    \n",
      "records     2.34e-02    1.21e-03\n",
      "released    3.28e-02    1.14e-03\n",
      "music       4.36e-02    2.12e-03\n",
      "band        5.63e-02    4.27e-03\n",
      "album       7.41e-02    4.96e-03\n",
      "\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "'''By EM'''\n",
    "visualize_EM_clusters(tf_idf, out['means'], out['covs'], map_index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Select all the topics that have a cluster in the model created above. [multiple choice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing to random initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create variables for randomly initializing the EM algorithm. Complete the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8.66001867   0.75293569  -2.78697115  13.70167582  -6.13612759]\n"
     ]
    }
   ],
   "source": [
    "print np.random.randn(5)*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(5) # See the note below to see why we set seed=5.\n",
    "num_clusters = len(means)\n",
    "num_docs, num_words = tf_idf.shape\n",
    "\n",
    "random_means = []\n",
    "random_covs = []\n",
    "random_weights = []\n",
    "\n",
    "for k in range(num_clusters):\n",
    "    \n",
    "    # Create a numpy array of length num_words with random normally distributed values.\n",
    "    # Use the standard univariate normal distribution (mean 0, variance 1).\n",
    "    # YOUR CODE HERE\n",
    "    mean = np.random.randn(num_words)\n",
    "    \n",
    "    # Create a numpy array of length num_words with random values uniformly distributed between 1 and 5.\n",
    "    # YOUR CODE HERE\n",
    "    cov = 5*np.random.randn(num_words) + 1.\n",
    "\n",
    "    # Initially give each cluster equal weight.\n",
    "    # YOUR CODE HERE\n",
    "    weight = 0.1\n",
    "    \n",
    "    random_means.append(mean)\n",
    "    random_covs.append(cov)\n",
    "    random_weights.append(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 0.44122749, -0.33087015,  2.43077119, ..., -0.34219415,\n",
      "       -1.57035633, -0.49190002]), array([-1.15593615, -0.307251  ,  0.35846375, ...,  0.17532613,\n",
      "       -0.19471828, -0.86139313]), array([ -1.17264022e-03,  -3.13909586e-01,   2.99073516e-02, ...,\n",
      "        -1.53438461e+00,  -2.43063811e+00,  -8.87695423e-01]), array([ 0.09222056,  1.78278706, -0.47566219, ..., -0.99481136,\n",
      "       -1.02602908, -0.24732317]), array([-0.78082584, -1.02219346, -0.56880082, ...,  1.37247112,\n",
      "        1.2696464 , -1.47166962]), array([ 0.75436271,  0.04673922,  0.7738681 , ...,  2.57508589,\n",
      "       -0.36601889, -1.60090859]), array([-2.53097684,  0.77194809,  1.3894362 , ...,  1.08989018,\n",
      "        0.0651108 , -0.26076514]), array([-0.91256716,  0.77017385, -0.56454252, ..., -0.06655384,\n",
      "       -0.28565974,  0.23027014]), array([-0.91126313, -1.19029827, -1.52965064, ..., -1.80314841,\n",
      "        0.5487375 ,  0.72153807]), array([ 1.32298234, -1.5554891 , -1.37455741, ..., -1.3175054 ,\n",
      "        1.82544922,  0.18939732]), array([-0.845098  , -0.51777202,  0.99157815, ..., -1.48990839,\n",
      "        0.44760452, -1.03231823]), array([ 0.98276673,  0.37782756,  0.08752024, ..., -0.61227963,\n",
      "       -0.29024746,  1.04426181]), array([-0.53291579,  0.85892296,  0.1600101 , ...,  0.36857574,\n",
      "       -2.55693961, -1.2192611 ]), array([-0.37483246, -0.43404863, -1.30878996, ..., -0.88242954,\n",
      "       -0.89307257,  0.96537042]), array([ 1.22692221, -0.23675591, -1.30873921, ...,  0.06566256,\n",
      "       -0.49223721,  0.63096662]), array([ 0.0451339 ,  0.45842004,  0.68016105, ...,  0.08228564,\n",
      "       -1.40175161, -0.87246298]), array([ 1.39245012,  1.67273201,  0.66556682, ..., -0.38928476,\n",
      "       -1.00659335, -0.71660401]), array([ 0.18308305, -0.97642016, -0.57597296, ..., -0.81203036,\n",
      "       -1.67507844,  0.97583784]), array([ 1.15220694, -0.91793706,  2.24484957, ..., -0.8400975 ,\n",
      "        0.43455892, -1.28723724]), array([ 1.14372135,  0.9755026 , -1.30068195, ...,  0.14078027,\n",
      "        1.61584906,  0.85136205]), array([-1.12770554, -0.54288381,  0.56082737, ..., -1.23075066,\n",
      "       -0.61124219,  0.34475139]), array([ 1.45341499, -0.10557431, -0.4258552 , ..., -0.63398843,\n",
      "       -1.92404426,  0.32861202]), array([ 0.5321129 , -0.57382372,  0.2541619 , ...,  1.44596072,\n",
      "        0.41996709,  1.37199366]), array([ 1.20274936, -2.44207716,  0.67405994, ...,  0.73224311,\n",
      "        0.82244842, -0.40181476]), array([-1.27209407,  0.75634737, -0.38648907, ..., -0.64902625,\n",
      "       -0.6594171 ,  0.70277766])]\n",
      "[array([-2.69005447, -6.97004631, -2.20879134, ...,  3.74998412,\n",
      "       -4.59048983,  1.70086988]), array([ 7.68830006, -3.43186448,  3.70895237, ...,  3.95924226,\n",
      "       -5.69725869,  3.03546561]), array([  1.25279518,  -0.44477739,   0.47669802, ...,   3.63775226,\n",
      "         4.34833179,  10.94680683]), array([ 2.29182027, -4.86105002,  1.25091534, ..., -3.72298699,\n",
      "        6.63809807, -1.15656521]), array([ -6.82341816,  14.25184099,  -2.96764009, ...,  -7.52422397,\n",
      "        -3.9407075 ,   1.0879746 ]), array([ 0.99205442, -3.6857922 , -3.95754762, ...,  5.08445986,\n",
      "        6.63324324,  6.22059812]), array([ 3.96160268,  2.53373989,  5.21984099, ...,  0.38780562,\n",
      "       -1.75981566,  1.97449497]), array([ 5.16300704, -3.0979206 ,  3.15360804, ..., -2.43618212,\n",
      "       -4.16254424,  9.11362236]), array([-3.66884087, -1.88420161,  5.9719967 , ...,  3.98628682,\n",
      "       -4.43256104,  2.05669485]), array([-1.25901041, -5.13443571,  3.54107393, ..., -2.31736941,\n",
      "       -3.26339184, -3.23721175]), array([ -0.46821057,   5.93202773,  14.02479018, ...,  -6.81638728,\n",
      "         5.70908605,  -6.6333903 ]), array([  2.49924686,   1.15545755,   5.87636847, ...,  15.5553661 ,\n",
      "         1.26320272,   0.99845373]), array([-3.17301176,  0.65731379, -4.20730152, ..., -6.26796331,\n",
      "       -0.30641593,  2.74691742]), array([ 1.30367073,  2.86381784, -9.4068527 , ...,  1.50273461,\n",
      "        2.94024156, -0.87333691]), array([ -1.78552621e+00,  -4.40135578e+00,  -1.09440740e+00, ...,\n",
      "        -4.67578850e-03,   2.75099981e-01,  -4.95937650e+00]), array([ 9.83549958, -8.12526108,  3.94012937, ...,  0.78970788,\n",
      "        2.08898026,  9.93204643]), array([ 13.2051839 ,   6.87696733,  -3.80873322, ...,   2.89965693,\n",
      "        -0.13148831,  -2.15648436]), array([ 2.44048112,  0.40856344,  0.09379283, ...,  1.85538568,\n",
      "        2.58670223,  1.23821332]), array([  3.7846057 ,   1.32693294,   9.18415876, ...,  -2.88548921,\n",
      "       -12.57455786,   9.16925097]), array([ 2.78118619, -0.06685782,  0.03996674, ..., -1.03368013,\n",
      "        2.20018617, -2.11326737]), array([ 6.63642091, -2.03498943, -2.76034659, ...,  6.81475229,\n",
      "       -0.52842288,  0.51235032]), array([ 0.6459386 ,  7.28932832, -0.06854286, ..., -1.66675877,\n",
      "        1.29255703, -8.19728727]), array([ 7.10301195, -4.66489484, -3.50700771, ...,  4.52355904,\n",
      "        0.9006192 , -8.5902628 ]), array([ 4.82147016,  5.49653326,  0.70406051, ..., -0.45348065,\n",
      "        2.86427243,  3.19289597]), array([  4.05150902,  10.24282324,  -4.05480549, ...,   2.59276347,\n",
      "         2.73104602,   9.78138707])]\n",
      "[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n"
     ]
    }
   ],
   "source": [
    "print random_means\n",
    "print random_covs\n",
    "print random_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Try fitting EM with the random initial parameters you created above. (Use `cov_smoothing=1e-5`.) Store the result to `out_random_init`. What is the final loglikelihood that the algorithm converges to? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-604091bd5891>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout_random_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEM_for_high_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_means\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_covs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_smoothing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mout_random_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loglik'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/damien/dev/python/ml_coursera/Course4/week4/assignment2/em_utilities.py\u001b[0m in \u001b[0;36mEM_for_high_dimension\u001b[0;34m(data, means, covs, weights, cov_smoothing, maxiter, thresh, verbose)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mlogresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mlogresp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlogpdf_diagonal_gaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSigma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mll_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_sum_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/damien/dev/python/ml_coursera/Course4/week4/assignment2/em_utilities.py\u001b[0m in \u001b[0;36mlogpdf_diagonal_gaussian\u001b[0;34m(x, mean, cov)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# sum of pairwise squared Eulidean distances gives SUM[(x_i - mean_i)^2/(2*sigma_i^2)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscaled_mean\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlog_sum_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0;31m# Special case to avoid picklability checks in delayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m     \u001b[0;31m# TODO: in some cases, backend='threading' may be appropriate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mpaired_distances\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdistances\u001b[0m \u001b[0mbetweens\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0mof\u001b[0m \u001b[0melements\u001b[0m \u001b[0mof\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_norm_squared\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         X = check_array(X, accept_sparse='csr', dtype=dtype,\n\u001b[0;32m--> 109\u001b[0;31m                         warn_on_dtype=warn_on_dtype, estimator=estimator)\n\u001b[0m\u001b[1;32m    110\u001b[0m         Y = check_array(Y, accept_sparse='csr', dtype=dtype,\n\u001b[1;32m    111\u001b[0m                         warn_on_dtype=warn_on_dtype, estimator=estimator)\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,\n\u001b[0;32m--> 380\u001b[0;31m                                       force_all_finite)\n\u001b[0m\u001b[1;32m    381\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite)\u001b[0m\n\u001b[1;32m    266\u001b[0m                           % spmatrix.format)\n\u001b[1;32m    267\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "out_random_init = EM_for_high_dimension(tf_idf, random_means, random_covs, random_weights, cov_smoothing=1e-5)\n",
    "out_random_init['loglik']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** Is the final loglikelihood larger or smaller than the final loglikelihood we obtained above when initializing EM with the results from running k-means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: For the above model, `out_random_init`, use the `visualize_EM_clusters` method you created above. Are the clusters more or less interpretable than the ones found after initializing using k-means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE. Use visualize_EM_clusters, which will require you to pass in tf_idf and map_index_to_word.\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Note**: Random initialization may sometimes produce a superior fit than k-means initialization. We do not claim that random initialization is always worse. However, this section does illustrate that random initialization often produces much worse clustering than k-means counterpart. This is the reason why we provide the particular random seed (`np.random.seed(5)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Takeaway\n",
    "\n",
    "In this assignment we were able to apply the EM algorithm to a mixture of Gaussians model of text data. This was made possible by modifying the model to assume a diagonal covariance for each cluster, and by modifying the implementation to use a sparse matrix representation. In the second part you explored the role of k-means initialization on the convergence of the model as well as the interpretability of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
